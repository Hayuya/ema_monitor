#!/usr/bin/env python3
"""
EMAæ‰¿èªç›£è¦–ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ - ãƒ‡ãƒãƒƒã‚°ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
EMAã‚µã‚¤ãƒˆã®æ§‹é€ ã‚’è©³ã—ãåˆ†æã™ã‚‹
"""

import requests
import logging
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

class EMADebugScraper:
    """EMAã‚µã‚¤ãƒˆã®ãƒ‡ãƒãƒƒã‚°ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.base_url = "https://www.ema.europa.eu"
        self.news_url = f"{self.base_url}/en/news"
        self.session = requests.Session()
        
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def debug_page_structure(self):
        """ãƒšãƒ¼ã‚¸æ§‹é€ ã‚’è©³ã—ãåˆ†æ"""
        try:
            print("=== EMAãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸æ§‹é€ åˆ†æ ===")
            
            response = self.session.get(self.news_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            print(f"âœ… ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ: {len(response.content)} ãƒã‚¤ãƒˆ")
            
            # åŸºæœ¬çš„ãªè¦ç´ ã®ç¢ºèª
            print(f"\nğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
            print(f"  - ç·è¦ç´ æ•°: {len(soup.find_all())}")
            print(f"  - divè¦ç´ æ•°: {len(soup.find_all('div'))}")
            print(f"  - ãƒªãƒ³ã‚¯æ•°: {len(soup.find_all('a'))}")
            print(f"  - è¦‹å‡ºã—æ•°: {len(soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']))}")
            
            # ã‚¯ãƒ©ã‚¹åã®åˆ†æ
            print(f"\nğŸ·ï¸ é‡è¦ãªã‚¯ãƒ©ã‚¹å:")
            class_patterns = ['view', 'content', 'news', 'item', 'article', 'listing', 'card']
            
            for pattern in class_patterns:
                elements = soup.find_all(attrs={'class': re.compile(pattern, re.I)})
                if elements:
                    print(f"  - '{pattern}' é–¢é€£: {len(elements)}å€‹")
                    # æœ€åˆã®è¦ç´ ã®ã‚¯ãƒ©ã‚¹åã‚’è¡¨ç¤º
                    if elements[0].get('class'):
                        print(f"    ä¾‹: {' '.join(elements[0]['class'])}")
            
            # view-contentã®è©³ç´°åˆ†æ
            view_content = soup.find('div', class_=re.compile(r'view-content'))
            if view_content:
                print(f"\nğŸ¯ view-contentåˆ†æ:")
                print(f"  - ã‚¯ãƒ©ã‚¹: {view_content.get('class')}")
                print(f"  - å­è¦ç´ æ•°: {len(view_content.find_all())}")
                print(f"  - ç›´æ¥ã®å­div: {len(view_content.find_all('div', recursive=False))}")
                print(f"  - å†…éƒ¨ã®ãƒªãƒ³ã‚¯: {len(view_content.find_all('a'))}")
                
                # æœ€åˆã®ã„ãã¤ã‹ã®ãƒªãƒ³ã‚¯ã‚’è©³ç´°è¡¨ç¤º
                links = view_content.find_all('a', href=True)[:5]
                print(f"\n  ğŸ“ æœ€åˆã®5å€‹ã®ãƒªãƒ³ã‚¯:")
                for i, link in enumerate(links, 1):
                    href = link.get('href')
                    text = link.get_text(strip=True)[:60]
                    print(f"    {i}. {text}...")
                    print(f"       URL: {href}")
                    
                    # è¦ªè¦ç´ ã®æƒ…å ±
                    parent = link.parent
                    if parent:
                        parent_class = parent.get('class', [])
                        print(f"       è¦ªè¦ç´ : {parent.name} (class: {parent_class})")
            
            # mainè¦ç´ ã®åˆ†æ
            main_element = soup.find('main')
            if main_element:
                print(f"\nğŸ“„ mainè¦ç´ åˆ†æ:")
                print(f"  - ã‚¯ãƒ©ã‚¹: {main_element.get('class')}")
                print(f"  - å­è¦ç´ æ•°: {len(main_element.find_all())}")
                
                # mainå†…ã®è¦‹å‡ºã—ã‚’æ¤œç´¢
                headings = main_element.find_all(['h1', 'h2', 'h3', 'h4'])
                print(f"  - è¦‹å‡ºã—æ•°: {len(headings)}")
                
                if headings:
                    print(f"  ğŸ“° æœ€åˆã®3å€‹ã®è¦‹å‡ºã—:")
                    for i, heading in enumerate(headings[:3], 1):
                        text = heading.get_text(strip=True)[:80]
                        print(f"    {i}. {heading.name}: {text}...")
                        
                        # é–¢é€£ã™ã‚‹ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
                        link = heading.find('a') or heading.find_next('a')
                        if link:
                            print(f"       ãƒªãƒ³ã‚¯: {link.get('href')}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def extract_sample_news(self):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡ºã—ã¦ãƒ†ã‚¹ãƒˆ"""
        try:
            print("\n=== ã‚µãƒ³ãƒ—ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ½å‡ºãƒ†ã‚¹ãƒˆ ===")
            
            response = self.session.get(self.news_url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # è¤‡æ•°ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡º
            approaches = [
                ("view-contentå†…ã®ãƒªãƒ³ã‚¯", self._extract_from_view_content),
                ("è¦‹å‡ºã—ãƒ™ãƒ¼ã‚¹", self._extract_from_headings),
                ("å…¨ãƒªãƒ³ã‚¯æ¤œç´¢", self._extract_from_all_links)
            ]
            
            for approach_name, extract_func in approaches:
                print(f"\nğŸ” {approach_name}ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:")
                items = extract_func(soup)
                print(f"  æŠ½å‡ºæ•°: {len(items)}ä»¶")
                
                for i, item in enumerate(items[:3], 1):
                    print(f"  {i}. {item['title'][:60]}...")
                    print(f"     URL: {item['link']}")
                    print(f"     æ‰¿èªé–¢é€£: {'ã¯ã„' if item['is_approval_related'] else 'ã„ã„ãˆ'}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ã‚µãƒ³ãƒ—ãƒ«æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _extract_from_view_content(self, soup):
        """view-contentã‹ã‚‰ã®æŠ½å‡º"""
        items = []
        view_content = soup.find('div', class_=re.compile(r'view-content'))
        if view_content:
            links = view_content.find_all('a', href=True)
            for i, link in enumerate(links[:10]):
                if '/news/' in link.get('href', ''):
                    item = self._create_item_from_link(link, f"vc_{i}")
                    if item:
                        items.append(item)
        return items
    
    def _extract_from_headings(self, soup):
        """è¦‹å‡ºã—ã‹ã‚‰ã®æŠ½å‡º"""
        items = []
        headings = soup.find_all(['h2', 'h3', 'h4'])
        for i, heading in enumerate(headings[:10]):
            link = heading.find('a') or heading.find_next('a')
            if link and link.get('href'):
                item = self._create_item_from_link(link, f"h_{i}")
                if item:
                    items.append(item)
        return items
    
    def _extract_from_all_links(self, soup):
        """å…¨ãƒªãƒ³ã‚¯ã‹ã‚‰ã®æŠ½å‡º"""
        items = []
        links = soup.find_all('a', href=True)
        processed = set()
        
        for i, link in enumerate(links):
            href = link.get('href', '')
            if ('/news/' in href and 
                href not in processed and 
                len(link.get_text(strip=True)) > 10):
                
                processed.add(href)
                item = self._create_item_from_link(link, f"all_{i}")
                if item:
                    items.append(item)
                    if len(items) >= 10:
                        break
        return items
    
    def _create_item_from_link(self, link, prefix):
        """ãƒªãƒ³ã‚¯ã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ ã‚’ä½œæˆ"""
        try:
            title = link.get_text(strip=True)
            if not title or len(title) < 10:
                return None
            
            href = link.get('href')
            full_url = urljoin(self.base_url, href)
            
            # æ‰¿èªé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
            approval_keywords = [
                'recommended for approval', 'positive opinion', 'marketing authorisation',
                'new medicine', 'chmp', 'committee for medicinal products',
                'approved', 'authorisation', 'recommendation', 'conditional marketing'
            ]
            
            is_approval_related = any(keyword in title.lower() for keyword in approval_keywords)
            
            return {
                'id': f"{prefix}_{hash(title + full_url)}",
                'title': title,
                'link': full_url,
                'date': "",
                'description': "",
                'is_approval_related': is_approval_related
            }
            
        except Exception as e:
            return None

def main():
    """ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸ” EMA ã‚µã‚¤ãƒˆæ§‹é€ ãƒ‡ãƒãƒƒã‚°ãƒ„ãƒ¼ãƒ«")
    print("=" * 50)
    
    scraper = EMADebugScraper()
    
    # ãƒšãƒ¼ã‚¸æ§‹é€ ã®åˆ†æ
    print("1ï¸âƒ£ ãƒšãƒ¼ã‚¸æ§‹é€ åˆ†æã‚’å®Ÿè¡Œä¸­...")
    success1 = scraper.debug_page_structure()
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ½å‡º
    print("\n2ï¸âƒ£ ã‚µãƒ³ãƒ—ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ½å‡ºã‚’å®Ÿè¡Œä¸­...")
    success2 = scraper.extract_sample_news()
    
    # çµæœã¾ã¨ã‚
    print("\n" + "=" * 50)
    print("ğŸ“Š ãƒ‡ãƒãƒƒã‚°çµæœã¾ã¨ã‚")
    print("=" * 50)
    
    if success1:
        print("âœ… ãƒšãƒ¼ã‚¸æ§‹é€ åˆ†æ: æˆåŠŸ")
    else:
        print("âŒ ãƒšãƒ¼ã‚¸æ§‹é€ åˆ†æ: å¤±æ•—")
    
    if success2:
        print("âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ½å‡º: æˆåŠŸ")
    else:
        print("âŒ ã‚µãƒ³ãƒ—ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ½å‡º: å¤±æ•—")
    
    if success1 and success2:
        print("\nğŸ‰ ãƒ‡ãƒãƒƒã‚°å®Œäº†ï¼ä¸Šè¨˜ã®æƒ…å ±ã‚’åŸºã«scraper.pyã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
    else:
        print("\nâš ï¸ å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚„ã‚µã‚¤ãƒˆæ§‹é€ ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    main()